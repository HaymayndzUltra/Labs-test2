---
description: "TAGS: [frontend,ui,component,popular] | TRIGGERS: frontend,ui,component,interface,web,popular | SCOPE: project-rules | DESCRIPTION: You are a Senior Front-End Developer and an Expert in ReactJS, NextJS, JavaScript, TypeScript, HTML,..."
alwaysApply: false
---

 # Process the response as it streams in
 async for line in response.aiter_lines():
 line = line.strip()
 if prompt_result:
 break
 # Empty line signals the end of an event
 if not line:
 if current_data:
 try:
 if current_event in ["log_message", "error"]:
 logging_callback(f"{current_event}: {current_data}")
 elif current_event == "prompt_result":
 prompt_result = json.loads(current_data)
 else:
 print(
 f"Unknown event: {current_event}, data: {current_data}"
 )
 except json.JSONDecodeError as e:
 print("Invalid JSON: ...")
 print(e)
 # Reset for next event
 current_data = ""
 current_event = "message"
 continue

# Parse SSE fields
 if line.startswith("event:"):
 current_event = line[6:].strip()
 elif line.startswith("data:"):
 current_data = line[5:].strip()
 elif line.startswith("id:"):
 # Handle event ID if needed
 pass
 elif line.startswith("retry:"):
 # Handle retry directive if needed
 pass
 return prompt_result

async def infer_with_logs(
 self,
 *,
 data: Dict[str, Any],
 logging_callback: Callable[[str], None],
 files: list[tuple[str, BufferedReader]] = [],
) -> Dict[str, Any] | None:
 if data.get("logs") is not True:
 raise Exception("Set the logs to True for streaming the process logs")

async with httpx.AsyncClient() as client:
 try:
 async with client.stream(
 "POST",
 self.infer_url,
 data=data,
 files=files,
 timeout=24000,
 follow_redirects=True,
 headers={
 "client_id": self.client_id,
 "client_secret": self.client_secret,
 },
 ) as response:
 if response.status_code == 201:
 # Check if it's actually a server-sent event stream
 if "text/event-stream" in response.headers.get(
 "content-type", ""
 ):
 prompt_result = await self.consume_event_source(
 response=response, logging_callback=logging_callback
 )
 return prompt_result
 else:
 # For non-SSE responses, read the content normally
 raise Exception(
 "Set the logs to True for streaming the process logs"
 )
 else:
 error_response = await response.aread()
 error_data = json.loads(error_response)
 raise Exception(
 f"API request failed with status {response.status_code}: {error_data}"
 )
 except Exception as e:
 raise Exception(f"Error with streaming request: {str(e)}")
def parse_parameters(params: dict): """ Parse parameters from a dictionary to a format suitable for the API call.

Args:
 params (dict): Dictionary of parameters

Returns:
 dict: Parsed parameters
"""
parsed_params = {}
files = []
for key, value in params.items():
 if isinstance(value, BufferedReader):
 files.append((key, value))
 else:
 parsed_params[key] = value
return parsed_params, files
async def infer( *, params: Dict[str, Any], api_url: str, override_workflow_api: Dict[str, Any] | None = None, client_id: str, client_secret: str, ): """ Make an inference with real-time logs from the execution prompt

Args:
 api_url (str): The URL to send the request to
 params (dict): The parameter to send to the workflow
 override_workflow_api (dict): Optional override the default workflow_api of the deployment

Returns:
 PromptResult: The result of the inference containing outputs and execution details
"""
client = ComfyAPIClient(
 infer_url=api_url,
 client_id=client_id,
 client_secret=client_secret,
)

params_parsed, files = parse_parameters(params)
data = {
 "logs": False,
 "params": json.dumps(params_parsed),
 "workflow_api": json.dumps(override_workflow_api)
 if override_workflow_api
 else None,
}
