---
TAGS: [AI,Project,Initialization,Setup,Structure]
TRIGGERS: ai project,initialize,setup,structure,boilerplate
SCOPE: project-initialization
DESCRIPTION: AI Project Initialization and Setup Rules
---

# AI Project Initialization Rules

## Primary Function: Standardized AI Project Setup

### Core Responsibilities
- **Project Structure Setup**: Create standardized directory structure for AI projects
- **Environment Configuration**: Set up development and deployment environments
- **Version Control Setup**: Initialize Git with proper configuration
- **Documentation Template**: Create comprehensive documentation templates
- **Project Planning**: Establish clear objectives, metrics, and timeline
- **Quality Assurance**: Ensure all AI projects follow best practices

## Mandatory Project Structure

### Directory Structure Requirements
```
ai-project-root/
├── src/                    # Source code
│   ├── __init__.py
│   ├── data/              # Data processing modules
│   │   ├── __init__.py
│   │   ├── preprocessing.py
│   │   ├── feature_engineering.py
│   │   └── data_validation.py
│   ├── models/            # Model definitions and training
│   │   ├── __init__.py
│   │   ├── base_model.py
│   │   ├── model_factory.py
│   │   └── training/
│   │       ├── __init__.py
│   │       ├── trainer.py
│   │       └── evaluator.py
│   ├── utils/             # Utility functions
│   │   ├── __init__.py
│   │   ├── helpers.py
│   │   ├── config.py
│   │   └── logging.py
│   └── api/               # API endpoints (if applicable)
│       ├── __init__.py
│       ├── routes.py
│       └── schemas.py
├── data/                  # Data storage
│   ├── raw/              # Raw data files
│   ├── processed/        # Processed data files
│   ├── external/         # External data sources
│   └── .gitkeep         # Keep empty directories in Git
├── models/               # Trained model storage
│   ├── checkpoints/     # Model checkpoints
│   ├── final/           # Final trained models
│   └── .gitkeep
├── notebooks/            # Jupyter notebooks
│   ├── 01_exploratory_data_analysis.ipynb
│   ├── 02_data_preprocessing.ipynb
│   ├── 03_model_development.ipynb
│   ├── 04_model_evaluation.ipynb
│   └── 05_deployment_analysis.ipynb
├── tests/                # Test files
│   ├── __init__.py
│   ├── test_data/
│   ├── test_models/
│   ├── test_utils/
│   └── conftest.py
├── config/               # Configuration files
│   ├── config.yaml
│   ├── model_config.yaml
│   └── logging_config.yaml
├── scripts/              # Utility scripts
│   ├── setup_environment.py
│   ├── download_data.py
│   ├── train_model.py
│   └── evaluate_model.py
├── docs/                 # Documentation
│   ├── api/
│   ├── architecture/
│   ├── deployment/
│   └── user_guide/
├── requirements.txt      # Python dependencies
├── requirements-dev.txt  # Development dependencies
├── pyproject.toml        # Project configuration
├── Dockerfile           # Container configuration
├── docker-compose.yml   # Multi-container setup
├── .env.example         # Environment variables template
├── .gitignore           # Git ignore rules
├── README.md            # Project documentation
├── CONTRIBUTING.md      # Contribution guidelines
├── LICENSE              # License file
└── .github/             # GitHub workflows
    └── workflows/
        ├── ci.yml
        ├── test.yml
        └── deploy.yml
```

## Environment Configuration Requirements

### Python Environment Setup
#### requirements.txt
```
# Core ML/AI libraries
numpy>=1.21.0
pandas>=1.3.0
scikit-learn>=1.0.0
tensorflow>=2.8.0
torch>=1.11.0
transformers>=4.15.0

# Data processing
matplotlib>=3.5.0
seaborn>=0.11.0
plotly>=5.0.0
jupyter>=1.0.0
ipykernel>=6.0.0

# Model serving
fastapi>=0.70.0
uvicorn>=0.15.0
pydantic>=1.8.0

# Database
sqlalchemy>=1.4.0
psycopg2-binary>=2.9.0

# Utilities
python-dotenv>=0.19.0
pyyaml>=6.0
tqdm>=4.62.0
click>=8.0.0
```

#### requirements-dev.txt
```
# Testing
pytest>=6.2.0
pytest-cov>=3.0.0
pytest-mock>=3.6.0
pytest-asyncio>=0.18.0

# Code quality
black>=21.0.0
flake8>=4.0.0
mypy>=0.910
isort>=5.9.0
pre-commit>=2.15.0

# Documentation
sphinx>=4.0.0
sphinx-rtd-theme>=1.0.0
mkdocs>=1.2.0

# Development tools
jupyterlab>=3.0.0
ipywidgets>=7.6.0
```

### Docker Configuration
#### Dockerfile
```dockerfile
FROM python:3.9-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy project files
COPY . .

# Create necessary directories
RUN mkdir -p data/raw data/processed models/checkpoints models/final

# Set environment variables
ENV PYTHONPATH=/app
ENV PYTHONUNBUFFERED=1

# Expose port
EXPOSE 8000

# Default command
CMD ["python", "scripts/train_model.py"]
```

#### docker-compose.yml
```yaml
version: '3.8'

services:
  ai-app:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data
      - ./models:/app/models
      - ./notebooks:/app/notebooks
    environment:
      - ENV=development
    env_file:
      - .env

  jupyter:
    build: .
    ports:
      - "8888:8888"
    volumes:
      - ./notebooks:/app/notebooks
      - ./data:/app/data
    command: jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root
    environment:
      - ENV=development
    env_file:
      - .env

  postgres:
    image: postgres:13
    environment:
      POSTGRES_DB: ai_project
      POSTGRES_USER: ai_user
      POSTGRES_PASSWORD: ai_password
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

volumes:
  postgres_data:
```

### Environment Variables
#### .env.example
```env
# Environment
ENV=development
DEBUG=True

# Database
DATABASE_URL=postgresql://ai_user:ai_password@localhost:5432/ai_project

# Model Configuration
MODEL_PATH=./models/final
CHECKPOINT_PATH=./models/checkpoints

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=1

# Logging
LOG_LEVEL=INFO
LOG_FILE=./logs/app.log

# Data Configuration
DATA_PATH=./data
RAW_DATA_PATH=./data/raw
PROCESSED_DATA_PATH=./data/processed

# External APIs
OPENAI_API_KEY=your_openai_api_key_here
HUGGINGFACE_API_KEY=your_huggingface_api_key_here
```

## Version Control Setup

### Git Configuration
#### .gitignore
```gitignore
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual Environment
venv/
env/
ENV/

# Jupyter Notebook
.ipynb_checkpoints

# Data files
data/raw/*
!data/raw/.gitkeep
data/processed/*
!data/processed/.gitkeep
data/external/*
!data/external/.gitkeep

# Model files
models/checkpoints/*
!models/checkpoints/.gitkeep
models/final/*
!models/final/.gitkeep

# Logs
logs/
*.log

# Environment variables
.env
.env.local
.env.production

# IDE
.vscode/
.idea/
*.swp
*.swo

# OS
.DS_Store
Thumbs.db

# Temporary files
*.tmp
*.temp
temp/
tmp/
```

### Git Initialization Commands
```bash
# Initialize Git repository
git init

# Add all files
git add .

# Initial commit
git commit -m "Initial commit: AI project setup"

# Add remote repository (if applicable)
git remote add origin <repository-url>

# Push to remote
git push -u origin main
```

## Documentation Templates

### README.md Template
```markdown
# AI Project Name

## Project Overview
Brief description of the AI project, its purpose, and main objectives.

## Features
- Feature 1
- Feature 2
- Feature 3

## Installation

### Prerequisites
- Python 3.9+
- Docker (optional)
- Git

### Setup
1. Clone the repository
```bash
git clone <repository-url>
cd ai-project-name
```

2. Create virtual environment
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies
```bash
pip install -r requirements.txt
pip install -r requirements-dev.txt
```

4. Set up environment variables
```bash
cp .env.example .env
# Edit .env with your configuration
```

5. Run the application
```bash
python scripts/train_model.py
```

## Project Structure
```
ai-project-root/
├── src/           # Source code
├── data/          # Data storage
├── models/        # Trained models
├── notebooks/     # Jupyter notebooks
├── tests/         # Test files
├── config/        # Configuration files
├── scripts/       # Utility scripts
└── docs/          # Documentation
```

## Usage
### Training a Model
```bash
python scripts/train_model.py --config config/model_config.yaml
```

### Running Tests
```bash
pytest tests/
```

### Starting Jupyter Lab
```bash
jupyter lab
```

## Configuration
Configuration files are located in the `config/` directory:
- `config.yaml` - Main configuration
- `model_config.yaml` - Model-specific configuration
- `logging_config.yaml` - Logging configuration

## Contributing
Please read [CONTRIBUTING.md](CONTRIBUTING.md) for details on our code of conduct and the process for submitting pull requests.

## License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Contact
Your Name - your.email@example.com
Project Link: [https://github.com/yourusername/ai-project-name](https://github.com/yourusername/ai-project-name)
```

### CONTRIBUTING.md Template
```markdown
# Contributing to AI Project Name

## Code of Conduct
This project and everyone participating in it is governed by our Code of Conduct.

## How to Contribute

### Reporting Bugs
- Use the issue tracker to report bugs
- Include detailed information about the bug
- Provide steps to reproduce the issue

### Suggesting Enhancements
- Use the issue tracker to suggest enhancements
- Clearly describe the enhancement
- Explain why it would be useful

### Pull Request Process
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests for your changes
5. Ensure all tests pass
6. Submit a pull request

### Development Setup
1. Follow the installation instructions in README.md
2. Install development dependencies
3. Run tests to ensure everything works
4. Create a new branch for your changes

### Coding Standards
- Follow PEP 8 for Python code
- Use type hints where appropriate
- Write docstrings for all functions and classes
- Add tests for new functionality

### Commit Messages
- Use clear, descriptive commit messages
- Reference issues in commit messages
- Use conventional commit format when possible

## Questions?
If you have questions, please open an issue or contact the maintainers.
```

## Project Planning Requirements

### Clear Objectives
#### Primary Objectives
- **Main Goal**: Clear statement of the primary objective
- **Success Criteria**: Measurable criteria for success
- **Target Metrics**: Specific metrics to achieve
- **Timeline**: Realistic timeline for completion

#### Secondary Objectives
- **Additional Goals**: Secondary objectives that support the main goal
- **Nice-to-Have Features**: Features that would be beneficial but not essential
- **Future Enhancements**: Potential future improvements

### Success Metrics
#### Technical Metrics
- **Model Performance**: Accuracy, precision, recall, F1-score
- **Processing Speed**: Training time, inference time
- **Resource Usage**: Memory usage, CPU usage, GPU usage
- **Code Quality**: Test coverage, code complexity, maintainability

#### Business Metrics
- **User Satisfaction**: User feedback scores, usage metrics
- **Cost Efficiency**: Cost per prediction, resource utilization
- **Scalability**: Ability to handle increased load
- **Reliability**: Uptime, error rates

### Timeline Planning
#### Phase 1: Setup and Data Preparation (Week 1-2)
- Project structure setup
- Environment configuration
- Data collection and preprocessing
- Initial exploratory data analysis

#### Phase 2: Model Development (Week 3-6)
- Model architecture design
- Training pipeline development
- Model training and validation
- Hyperparameter tuning

#### Phase 3: Evaluation and Optimization (Week 7-8)
- Model evaluation and testing
- Performance optimization
- Error analysis and debugging
- Documentation completion

#### Phase 4: Deployment and Monitoring (Week 9-10)
- Model deployment
- Monitoring setup
- Performance tracking
- Maintenance planning

## Quality Assurance

### Code Quality Standards
- **PEP 8 Compliance**: Follow Python style guidelines
- **Type Hints**: Use type hints for all functions
- **Docstrings**: Document all functions and classes
- **Error Handling**: Implement proper error handling
- **Logging**: Use structured logging throughout

### Testing Requirements
- **Unit Tests**: Test individual functions and methods
- **Integration Tests**: Test component interactions
- **End-to-End Tests**: Test complete workflows
- **Performance Tests**: Test performance under load
- **Test Coverage**: Maintain minimum 80% test coverage

### Documentation Standards
- **Code Documentation**: Clear docstrings and comments
- **API Documentation**: Document all APIs and interfaces
- **User Documentation**: Clear user guides and tutorials
- **Architecture Documentation**: Document system architecture
- **Deployment Documentation**: Document deployment procedures

## Remember
**I am the AI Project Initialization System. My job is to ensure all AI projects start with proper structure, configuration, documentation, and planning. I create comprehensive boilerplate that follows best practices and sets projects up for success.**