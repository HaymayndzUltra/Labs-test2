---
description: "TAGS: [api,testing,automation,postman,newman] | TRIGGERS: [api,test,postman,newman,automation,quality-gate] | SCOPE: global | DESCRIPTION: Automated API testing with Postman and Newman for quality gates and evidence collection"
---

# API Testing Automation with One-liner Scripts

## Core Requirements [STRICT]

### 1. Package.json Scripts
- **ALWAYS** include `api:run:stage` script for running Postman tests
- **ALWAYS** include `api:gate` script for quality gate validation
- **ALWAYS** use Newman for command-line Postman execution
- **ALWAYS** generate both CLI and JSON reports

### 2. Quality Gate Implementation
- **MUST** exit with code 0 for success, 1 for failure
- **MUST** validate all test assertions passed
- **MUST** check response time thresholds
- **MUST** generate evidence file for audit trails
- **MUST** provide detailed failure information

### 3. Evidence Collection
- **ALWAYS** save test results to JSON file
- **ALWAYS** generate human-readable evidence file
- **ALWAYS** include timestamps and test statistics
- **ALWAYS** store evidence in `clients/{client}/reports/api/` directory

## Implementation Guidelines [GUIDELINE]

### Script Structure
```json
{
  "scripts": {
    "api:run:stage": "newman run clients/{client}/postman/collection.json -e clients/{client}/postman/env.stage.json --reporters cli,json --reporter-json-export clients/{client}/reports/api/postman_stage.json",
    "api:gate": "node tools/check-postman.js clients/{client}/reports/api/postman_stage.json"
  }
}
```

### Quality Gate Checker Features
- Parse JSON report and extract statistics
- Validate all assertions passed
- Check performance thresholds
- Display detailed test summary
- Generate evidence file with timestamps
- Exit with appropriate status codes

### Test Collection Requirements
- Include health check endpoints
- Test authentication and authorization
- Validate CRUD operations
- Check response times and status codes
- Include error handling tests
- Use environment variables for configuration

## File Organization [STRICT]

### Directory Structure
```
clients/
└── {client-name}/
    ├── postman/
    │   ├── collection.json      # Postman test collection
    │   └── env.stage.json      # Stage environment variables
    └── reports/
        └── api/                # Test reports and evidence
            ├── postman_stage.json
            └── test_evidence.txt
```

### Tools Directory
```
tools/
└── check-postman.js           # Quality gate checker script
```

## Quality Gate Rules [STRICT]

### Pass Conditions
- ✅ All test assertions must pass
- ✅ Response times within thresholds
- ✅ Required fields present in responses
- ✅ Status codes match expectations
- ✅ No critical errors detected

### Fail Conditions
- ❌ Any test assertion fails
- ❌ Response time exceeds threshold
- ❌ Missing required fields
- ❌ Incorrect status codes
- ❌ Critical errors detected

## Evidence Collection [STRICT]

### JSON Report Format
```json
{
  "run": {
    "stats": {
      "assertions": {
        "total": 9,
        "failed": 0
      }
    },
    "timings": {
      "started": 1234567890,
      "completed": 1234567891
    },
    "failures": []
  }
}
```

### Evidence File Format
```
Postman Test Evidence
====================
Date: {timestamp}
Report: {report_path}
Status: {PASSED/FAILED}
Tests: {total_tests}
Passed: {passed_tests}
Failed: {failed_tests}
Duration: {duration}ms
Quality Gate: {PASSED/FAILED}
```

## CI/CD Integration [GUIDELINE]

### Pipeline Integration
- Run `api:run:stage` before deployment
- Run `api:gate` as quality gate
- Block deployment if quality gate fails
- Store evidence files for audit trails
- Send notifications on test failures

### Error Handling
- Handle missing report files gracefully
- Provide clear error messages
- Log detailed failure information
- Support debugging and troubleshooting

## Performance Monitoring [GUIDELINE]

### Response Time Thresholds
- Health checks: < 1000ms
- API endpoints: < 2000ms
- Database operations: < 3000ms
- File uploads: < 5000ms

### Monitoring Features
- Track response times per endpoint
- Monitor test execution duration
- Alert on performance degradation
- Generate performance reports

## Security Considerations [STRICT]

### Environment Variables
- **NEVER** commit sensitive tokens to version control
- **ALWAYS** use environment-specific configuration files
- **ALWAYS** validate authentication tokens
- **ALWAYS** use secure storage for credentials

### Test Data
- Use test-specific data only
- Avoid production data in tests
- Clean up test data after execution
- Validate data privacy compliance

## Best Practices [GUIDELINE]

### Test Design
- Write comprehensive test scenarios
- Include positive and negative test cases
- Test edge cases and error conditions
- Use descriptive test names and assertions

### Maintenance
- Keep test collections up to date
- Regularly review and update tests
- Monitor test execution performance
- Document test requirements and changes

### Team Collaboration
- Share test results with team
- Document test procedures
- Provide training on test automation
- Establish testing standards and guidelines

## Troubleshooting [GUIDELINE]

### Common Issues
- Report file not found: Check if `api:run:stage` ran successfully
- Tests failing: Review assertions and environment variables
- Permission errors: Ensure scripts are executable
- Timeout issues: Check network connectivity and API availability

### Debug Mode
- Use `DEBUG=*` for verbose output
- Check individual report files manually
- Validate environment configuration
- Test individual API endpoints

## Integration Examples [GUIDELINE]

### GitHub Actions
```yaml
- name: Run API Tests
  run: npm run api:run:stage

- name: Quality Gate Check
  run: npm run api:gate
```

### Docker Integration
```dockerfile
COPY package.json ./
RUN npm install
COPY clients/ ./clients/
RUN npm run api:run:stage && npm run api:gate
```

### Slack Notifications
```javascript
// Add to check-postman.js
if (stats.assertions.failed > 0) {
  // Send Slack notification
  sendSlackAlert('API tests failed', failures);
}
```

This rule ensures consistent, automated API testing with quality gates and evidence collection across all projects.