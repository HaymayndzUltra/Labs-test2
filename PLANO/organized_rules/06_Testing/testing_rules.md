# Testing Rules

## 1. Test Coverage Requirements
```
"Create a rule that all new code must have 90% test coverage, include unit tests, integration tests, and edge case testing"
```

## 2. Test Data Management
```
"Create a rule that all tests must use fixtures, avoid hardcoded data, and include setup/teardown for database state"
```

## 3. Mock and Stub Standards
```
"Create a rule that all external dependencies must be mocked in tests, and mocks must be reset between tests"
```

## 4. AI Testing Strategy
```
"Create a rule that all AI projects must implement comprehensive testing: unit tests (individual functions, classes), integration tests (data pipelines, APIs), model tests (accuracy, performance, edge cases), end-to-end tests (complete workflows), and regression tests (model updates, data changes)"
```

## 5. Data Quality Testing
```
"Create a rule that all AI projects must have data quality testing: data validation tests (schema, format, completeness), data quality metrics (accuracy, consistency, timeliness), data drift tests (statistical tests, distribution changes), and data privacy tests (anonymization, encryption, access controls)"
```

## 6. Model Quality Testing
```
"Create a rule that all AI projects must implement model quality testing: accuracy testing (cross-validation, holdout testing), performance testing (latency, throughput, resource usage), robustness testing (adversarial attacks, edge cases), and bias testing (fairness metrics, demographic parity)"
```

## 7. Test Automation
```
"Create a rule that all tests must be automated using Selenium, include proper test data management, have parallel execution, support cross-browser testing, and include visual regression testing"
```

## 8. Quality Gates
```
"Create a rule that all code must pass quality gates including code coverage, security scanning, performance benchmarks, accessibility compliance, and user acceptance testing"
```

## 9. Testing Strategy
```
"Create a rule that all testing must include unit tests, integration tests, end-to-end tests, performance tests, security tests, and accessibility tests with proper coverage metrics"
```

## 10. AI Model Testing
```
"Create a rule that all ML models must be versioned with MLflow, include data validation with Great Expectations, have model performance monitoring, and store artifacts in S3 with proper naming convention: models/{project}/{version}/{model_name}.pkl"
```

## 11. Continuous Testing
```
"Create a rule that all AI projects must implement continuous learning: model retraining (scheduled, triggered), data updates (new data, improved quality), model updates (new architectures, hyperparameters), and performance monitoring (metrics, feedback loops)"
```
