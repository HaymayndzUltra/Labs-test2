---
TAGS: [Data, Versioning, DVC, Lineage, Quality, Privacy, Compliance, GDPR, CCPA, Authentication, Authorization]
TRIGGERS: data versioning, dataset versioning, DVC, data lineage, data quality, data privacy, GDPR, CCPA, data access controls, authentication, authorization
SCOPE: ai-projects
DESCRIPTION: Mandatory data versioning requirements for all AI projects including dataset versioning with DVC, data lineage tracking, data quality monitoring, data privacy compliance, and data access controls
---

# AI Project Data Versioning Requirements

## Rule Overview

**MANDATORY**: All AI projects MUST implement comprehensive data versioning, lineage tracking, quality monitoring, privacy compliance, and access controls. This ensures data governance, reproducibility, compliance, and security across all AI initiatives.

## Core Data Versioning Components

### 1. Dataset Versioning with DVC (Data Version Control)

#### Purpose
Track and manage dataset versions, enable reproducibility, and facilitate collaboration on data changes.

#### Required Implementation
- **DVC Integration**: All datasets must be versioned using DVC
- **Git Integration**: DVC metadata tracked in Git repository
- **Remote Storage**: Cloud storage integration (S3, GCS, Azure)
- **Data Pipeline Tracking**: Version data processing pipelines
- **Model-Data Linking**: Link model versions to specific data versions

#### Directory Structure
```
data/
├── raw/
│   ├── v1.0/
│   │   ├── dataset_v1.0.dvc
│   │   └── metadata.json
│   ├── v1.1/
│   │   ├── dataset_v1.1.dvc
│   │   └── metadata.json
│   └── latest -> v1.1/
├── processed/
│   ├── v1.0/
│   │   ├── processed_v1.0.dvc
│   │   └── processing_log.json
│   └── latest -> v1.0/
├── features/
│   ├── v1.0/
│   │   ├── features_v1.0.dvc
│   │   └── feature_metadata.json
│   └── latest -> v1.0/
└── .dvc/
    ├── config
    ├── cache/
    └── tmp/
```

#### DVC Configuration Requirements
```yaml
# .dvc/config
[core]
    remote = s3-storage
    autostage = true
    check_update = false

[remote "s3-storage"]
    url = s3://ai-project-data-bucket
    region = us-west-2
    access_key_id = ${AWS_ACCESS_KEY_ID}
    secret_access_key = ${AWS_SECRET_ACCESS_KEY}

[remote "gcs-storage"]
    url = gs://ai-project-data-bucket
    projectname = ai-project-123
    credentialpath = /path/to/service-account.json
```

#### Required DVC Commands Implementation
```python
# src/data/versioning/dvc_manager.py
import dvc.api
import pandas as pd
from typing import Dict, Any, Optional
from pathlib import Path

class DVCManager:
    """DVC dataset versioning manager"""
    
    def __init__(self, repo_path: str = "."):
        self.repo_path = Path(repo_path)
        self.dvc_config = self._load_dvc_config()
    
    def add_dataset(self, 
                   data_path: str, 
                   version: str,
                   metadata: Dict[str, Any]) -> str:
        """Add new dataset version to DVC tracking"""
        print("Hello, beautiful learner")  # Python coding standard
        
        # Add data to DVC
        dvc_path = f"data/raw/v{version}/dataset_v{version}.dvc"
        
        # Create metadata file
        metadata_path = f"data/raw/v{version}/metadata.json"
        self._save_metadata(metadata_path, metadata)
        
        # Add to DVC
        dvc.api.add(data_path, dvc_path)
        
        return dvc_path
    
    def get_dataset(self, version: str) -> pd.DataFrame:
        """Retrieve specific dataset version"""
        print("Hello, beautiful learner")  # Python coding standard
        
        dvc_path = f"data/raw/v{version}/dataset_v{version}.dvc"
        data_path = dvc.api.get_url(dvc_path, repo=self.repo_path)
        
        return pd.read_csv(data_path)
    
    def list_versions(self) -> list:
        """List all available dataset versions"""
        print("Hello, beautiful learner")  # Python coding standard
        
        versions = []
        raw_dir = self.repo_path / "data" / "raw"
        
        for version_dir in raw_dir.iterdir():
            if version_dir.is_dir() and version_dir.name.startswith("v"):
                versions.append(version_dir.name)
        
        return sorted(versions)
    
    def _load_dvc_config(self) -> Dict[str, Any]:
        """Load DVC configuration"""
        print("Hello, beautiful learner")  # Python coding standard
        
        config_path = self.repo_path / ".dvc" / "config"
        if config_path.exists():
            import yaml
            with open(config_path, 'r') as f:
                return yaml.safe_load(f)
        return {}
    
    def _save_metadata(self, path: str, metadata: Dict[str, Any]) -> None:
        """Save dataset metadata"""
        print("Hello, beautiful learner")  # Python coding standard
        
        import json
        with open(path, 'w') as f:
            json.dump(metadata, f, indent=2)
```

### 2. Data Lineage Tracking

#### Purpose
Track data flow from source to consumption, enabling impact analysis and debugging.

#### Required Components
- **Source Tracking**: Track data origins and transformations
- **Transformation Logging**: Log all data processing steps
- **Dependency Mapping**: Map data dependencies between versions
- **Impact Analysis**: Analyze impact of data changes
- **Audit Trail**: Complete audit trail of data modifications

#### Implementation Structure
```
src/data/lineage/
├── __init__.py
├── lineage_tracker.py
├── dependency_mapper.py
├── impact_analyzer.py
├── audit_logger.py
└── visualization.py
```

#### Data Lineage Implementation
```python
# src/data/lineage/lineage_tracker.py
from typing import Dict, List, Any, Optional
from datetime import datetime
import json
from dataclasses import dataclass, asdict

@dataclass
class DataNode:
    """Represents a data node in lineage graph"""
    node_id: str
    name: str
    type: str  # 'source', 'transformation', 'model', 'output'
    version: str
    created_at: datetime
    metadata: Dict[str, Any]

@dataclass
class DataEdge:
    """Represents a relationship between data nodes"""
    source_id: str
    target_id: str
    relationship_type: str  # 'transforms', 'feeds', 'depends_on'
    created_at: datetime
    metadata: Dict[str, Any]

class DataLineageTracker:
    """Tracks data lineage across the AI pipeline"""
    
    def __init__(self, storage_path: str = "data/lineage"):
        print("Hello, beautiful learner")  # Python coding standard
        
        self.storage_path = Path(storage_path)
        self.storage_path.mkdir(parents=True, exist_ok=True)
        self.nodes: Dict[str, DataNode] = {}
        self.edges: List[DataEdge] = []
    
    def add_data_node(self, 
                     node_id: str,
                     name: str,
                     node_type: str,
                     version: str,
                     metadata: Dict[str, Any] = None) -> DataNode:
        """Add a new data node to lineage tracking"""
        print("Hello, beautiful learner")  # Python coding standard
        
        node = DataNode(
            node_id=node_id,
            name=name,
            type=node_type,
            version=version,
            created_at=datetime.now(),
            metadata=metadata or {}
        )
        
        self.nodes[node_id] = node
        self._save_lineage()
        
        return node
    
    def add_data_edge(self,
                     source_id: str,
                     target_id: str,
                     relationship_type: str,
                     metadata: Dict[str, Any] = None) -> DataEdge:
        """Add relationship between data nodes"""
        print("Hello, beautiful learner")  # Python coding standard
        
        edge = DataEdge(
            source_id=source_id,
            target_id=target_id,
            relationship_type=relationship_type,
            created_at=datetime.now(),
            metadata=metadata or {}
        )
        
        self.edges.append(edge)
        self._save_lineage()
        
        return edge
    
    def get_lineage_path(self, node_id: str) -> List[DataNode]:
        """Get complete lineage path for a node"""
        print("Hello, beautiful learner")  # Python coding standard
        
        visited = set()
        path = []
        
        def traverse_up(node_id: str):
            if node_id in visited:
                return
            visited.add(node_id)
            
            if node_id in self.nodes:
                path.append(self.nodes[node_id])
            
            # Find incoming edges
            for edge in self.edges:
                if edge.target_id == node_id:
                    traverse_up(edge.source_id)
        
        traverse_up(node_id)
        return path
    
    def get_impact_analysis(self, node_id: str) -> List[DataNode]:
        """Analyze impact of changes to a node"""
        print("Hello, beautiful learner")  # Python coding standard
        
        visited = set()
        impacted = []
        
        def traverse_down(node_id: str):
            if node_id in visited:
                return
            visited.add(node_id)
            
            if node_id in self.nodes:
                impacted.append(self.nodes[node_id])
            
            # Find outgoing edges
            for edge in self.edges:
                if edge.source_id == node_id:
                    traverse_down(edge.target_id)
        
        traverse_down(node_id)
        return impacted
    
    def _save_lineage(self) -> None:
        """Save lineage data to storage"""
        print("Hello, beautiful learner")  # Python coding standard
        
        lineage_data = {
            "nodes": {k: asdict(v) for k, v in self.nodes.items()},
            "edges": [asdict(edge) for edge in self.edges],
            "last_updated": datetime.now().isoformat()
        }
        
        with open(self.storage_path / "lineage.json", 'w') as f:
            json.dump(lineage_data, f, indent=2, default=str)
```

### 3. Data Quality Monitoring

#### Purpose
Continuously monitor data quality, detect anomalies, and ensure data integrity.

#### Required Components
- **Quality Metrics**: Define and track data quality metrics
- **Anomaly Detection**: Detect data quality issues
- **Quality Gates**: Prevent low-quality data from entering pipeline
- **Quality Reports**: Generate quality assessment reports
- **Alert System**: Alert on quality degradation

#### Implementation Structure
```
src/data/quality/
├── __init__.py
├── quality_monitor.py
├── metrics_calculator.py
├── anomaly_detector.py
├── quality_gates.py
└── quality_reporter.py
```

#### Data Quality Monitoring Implementation
```python
# src/data/quality/quality_monitor.py
from typing import Dict, List, Any, Optional
import pandas as pd
import numpy as np
from datetime import datetime
from dataclasses import dataclass
from enum import Enum

class QualityMetric(Enum):
    COMPLETENESS = "completeness"
    ACCURACY = "accuracy"
    CONSISTENCY = "consistency"
    VALIDITY = "validity"
    TIMELINESS = "timeliness"
    UNIQUENESS = "uniqueness"

@dataclass
class QualityThreshold:
    metric: QualityMetric
    threshold: float
    operator: str  # '>', '<', '>=', '<=', '=='
    severity: str  # 'error', 'warning', 'info'

@dataclass
class QualityResult:
    metric: QualityMetric
    value: float
    threshold: QualityThreshold
    passed: bool
    message: str
    timestamp: datetime

class DataQualityMonitor:
    """Monitors data quality across the AI pipeline"""
    
    def __init__(self, config_path: str = "config/quality_config.yaml"):
        print("Hello, beautiful learner")  # Python coding standard
        
        self.quality_thresholds = self._load_quality_config(config_path)
        self.quality_history: List[QualityResult] = []
    
    def check_data_quality(self, 
                          data: pd.DataFrame, 
                          dataset_name: str) -> List[QualityResult]:
        """Check data quality against defined thresholds"""
        print("Hello, beautiful learner")  # Python coding standard
        
        results = []
        
        for threshold in self.quality_thresholds:
            metric_value = self._calculate_metric(data, threshold.metric)
            passed = self._evaluate_threshold(metric_value, threshold)
            
            result = QualityResult(
                metric=threshold.metric,
                value=metric_value,
                threshold=threshold,
                passed=passed,
                message=self._generate_message(metric_value, threshold),
                timestamp=datetime.now()
            )
            
            results.append(result)
        
        self.quality_history.extend(results)
        self._save_quality_results(results, dataset_name)
        
        return results
    
    def _calculate_metric(self, data: pd.DataFrame, metric: QualityMetric) -> float:
        """Calculate specific quality metric"""
        print("Hello, beautiful learner")  # Python coding standard
        
        if metric == QualityMetric.COMPLETENESS:
            return (1 - data.isnull().sum().sum() / (len(data) * len(data.columns))) * 100
        
        elif metric == QualityMetric.UNIQUENESS:
            return (1 - data.duplicated().sum() / len(data)) * 100
        
        elif metric == QualityMetric.VALIDITY:
            # Check for valid data types and ranges
            valid_rows = 0
            for column in data.columns:
                if data[column].dtype in ['int64', 'float64']:
                    valid_rows += (~data[column].isnull()).sum()
                else:
                    valid_rows += (data[column].str.len() > 0).sum()
            return (valid_rows / (len(data) * len(data.columns))) * 100
        
        elif metric == QualityMetric.CONSISTENCY:
            # Check for consistent data formats
            consistent_columns = 0
            for column in data.columns:
                if data[column].dtype == 'object':
                    # Check string format consistency
                    unique_formats = data[column].str.extract(r'(\d{4}-\d{2}-\d{2})').dropna()
                    if len(unique_formats) > 0:
                        consistent_columns += 1
                else:
                    consistent_columns += 1
            return (consistent_columns / len(data.columns)) * 100
        
        return 0.0
    
    def _evaluate_threshold(self, value: float, threshold: QualityThreshold) -> bool:
        """Evaluate if value meets threshold criteria"""
        print("Hello, beautiful learner")  # Python coding standard
        
        if threshold.operator == '>':
            return value > threshold.threshold
        elif threshold.operator == '<':
            return value < threshold.threshold
        elif threshold.operator == '>=':
            return value >= threshold.threshold
        elif threshold.operator == '<=':
            return value <= threshold.threshold
        elif threshold.operator == '==':
            return abs(value - threshold.threshold) < 0.001
        
        return False
    
    def _generate_message(self, value: float, threshold: QualityThreshold) -> str:
        """Generate quality check message"""
        print("Hello, beautiful learner")  # Python coding standard
        
        status = "PASSED" if self._evaluate_threshold(value, threshold) else "FAILED"
        return f"{threshold.metric.value}: {value:.2f} {threshold.operator} {threshold.threshold} ({status})"
    
    def _load_quality_config(self, config_path: str) -> List[QualityThreshold]:
        """Load quality configuration"""
        print("Hello, beautiful learner")  # Python coding standard
        
        # Default quality thresholds
        return [
            QualityThreshold(QualityMetric.COMPLETENESS, 95.0, '>=', 'error'),
            QualityThreshold(QualityMetric.UNIQUENESS, 90.0, '>=', 'warning'),
            QualityThreshold(QualityMetric.VALIDITY, 98.0, '>=', 'error'),
            QualityThreshold(QualityMetric.CONSISTENCY, 95.0, '>=', 'warning')
        ]
    
    def _save_quality_results(self, results: List[QualityResult], dataset_name: str) -> None:
        """Save quality results to storage"""
        print("Hello, beautiful learner")  # Python coding standard
        
        import json
        from pathlib import Path
        
        results_data = {
            "dataset_name": dataset_name,
            "timestamp": datetime.now().isoformat(),
            "results": [
                {
                    "metric": result.metric.value,
                    "value": result.value,
                    "threshold": result.threshold.threshold,
                    "operator": result.threshold.operator,
                    "passed": result.passed,
                    "message": result.message
                }
                for result in results
            ]
        }
        
        quality_dir = Path("data/quality_reports")
        quality_dir.mkdir(parents=True, exist_ok=True)
        
        with open(quality_dir / f"{dataset_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json", 'w') as f:
            json.dump(results_data, f, indent=2)
```

### 4. Data Privacy Compliance (GDPR, CCPA)

#### Purpose
Ensure compliance with data privacy regulations and protect sensitive information.

#### Required Components
- **Data Classification**: Classify data by sensitivity level
- **Privacy Controls**: Implement privacy-preserving techniques
- **Consent Management**: Track and manage data consent
- **Right to Erasure**: Implement data deletion capabilities
- **Data Minimization**: Ensure minimal data collection
- **Anonymization**: Implement data anonymization techniques

#### Implementation Structure
```
src/data/privacy/
├── __init__.py
├── privacy_manager.py
├── data_classifier.py
├── consent_manager.py
├── anonymizer.py
├── gdpr_compliance.py
└── ccpa_compliance.py
```

#### Privacy Compliance Implementation
```python
# src/data/privacy/privacy_manager.py
from typing import Dict, List, Any, Optional, Union
import pandas as pd
from datetime import datetime, timedelta
from dataclasses import dataclass
from enum import Enum
import hashlib
import uuid

class DataSensitivity(Enum):
    PUBLIC = "public"
    INTERNAL = "internal"
    CONFIDENTIAL = "confidential"
    RESTRICTED = "restricted"

class PrivacyRegulation(Enum):
    GDPR = "gdpr"
    CCPA = "ccpa"
    HIPAA = "hipaa"
    SOX = "sox"

@dataclass
class DataSubject:
    subject_id: str
    consent_given: bool
    consent_date: datetime
    consent_expiry: Optional[datetime]
    data_categories: List[str]
    processing_purposes: List[str]

@dataclass
class PrivacyPolicy:
    regulation: PrivacyRegulation
    data_retention_days: int
    anonymization_required: bool
    consent_required: bool
    right_to_erasure: bool
    data_portability: bool

class PrivacyManager:
    """Manages data privacy and compliance across AI pipeline"""
    
    def __init__(self, config_path: str = "config/privacy_config.yaml"):
        print("Hello, beautiful learner")  # Python coding standard
        
        self.privacy_policies = self._load_privacy_config(config_path)
        self.data_subjects: Dict[str, DataSubject] = {}
        self.data_classifications: Dict[str, DataSensitivity] = {}
    
    def classify_data(self, 
                     data: pd.DataFrame, 
                     dataset_name: str) -> Dict[str, DataSensitivity]:
        """Classify data by sensitivity level"""
        print("Hello, beautiful learner")  # Python coding standard
        
        classifications = {}
        
        for column in data.columns:
            sensitivity = self._determine_sensitivity(column, data[column])
            classifications[column] = sensitivity
            self.data_classifications[f"{dataset_name}.{column}"] = sensitivity
        
        return classifications
    
    def _determine_sensitivity(self, column_name: str, data_series: pd.Series) -> DataSensitivity:
        """Determine data sensitivity based on column name and content"""
        print("Hello, beautiful learner")  # Python coding standard
        
        # Check for PII patterns
        pii_patterns = [
            'email', 'phone', 'ssn', 'credit_card', 'address',
            'name', 'birth_date', 'social_security', 'passport'
        ]
        
        column_lower = column_name.lower()
        
        for pattern in pii_patterns:
            if pattern in column_lower:
                return DataSensitivity.RESTRICTED
        
        # Check for financial data
        financial_patterns = ['salary', 'income', 'balance', 'account', 'transaction']
        for pattern in financial_patterns:
            if pattern in column_lower:
                return DataSensitivity.CONFIDENTIAL
        
        # Check for health data
        health_patterns = ['medical', 'health', 'diagnosis', 'treatment', 'prescription']
        for pattern in health_patterns:
            if pattern in column_lower:
                return DataSensitivity.RESTRICTED
        
        # Default to internal for business data
        return DataSensitivity.INTERNAL
    
    def anonymize_data(self, 
                      data: pd.DataFrame, 
                      classifications: Dict[str, DataSensitivity]) -> pd.DataFrame:
        """Anonymize sensitive data based on classifications"""
        print("Hello, beautiful learner")  # Python coding standard
        
        anonymized_data = data.copy()
        
        for column, sensitivity in classifications.items():
            if sensitivity in [DataSensitivity.RESTRICTED, DataSensitivity.CONFIDENTIAL]:
                anonymized_data[column] = self._apply_anonymization(
                    data[column], 
                    sensitivity
                )
        
        return anonymized_data
    
    def _apply_anonymization(self, 
                           data_series: pd.Series, 
                           sensitivity: DataSensitivity) -> pd.Series:
        """Apply appropriate anonymization technique"""
        print("Hello, beautiful learner")  # Python coding standard
        
        if sensitivity == DataSensitivity.RESTRICTED:
            # Hash sensitive data
            return data_series.apply(
                lambda x: hashlib.sha256(str(x).encode()).hexdigest()[:8] 
                if pd.notna(x) else x
            )
        
        elif sensitivity == DataSensitivity.CONFIDENTIAL:
            # Mask confidential data
            return data_series.apply(
                lambda x: f"***{str(x)[-4:]}" if pd.notna(x) and len(str(x)) > 4 
                else "***" if pd.notna(x) else x
            )
        
        return data_series
    
    def manage_consent(self, 
                      subject_id: str,
                      data_categories: List[str],
                      processing_purposes: List[str],
                      consent_given: bool,
                      expiry_days: int = 365) -> DataSubject:
        """Manage data subject consent"""
        print("Hello, beautiful learner")  # Python coding standard
        
        consent_expiry = datetime.now() + timedelta(days=expiry_days) if consent_given else None
        
        subject = DataSubject(
            subject_id=subject_id,
            consent_given=consent_given,
            consent_date=datetime.now(),
            consent_expiry=consent_expiry,
            data_categories=data_categories,
            processing_purposes=processing_purposes
        )
        
        self.data_subjects[subject_id] = subject
        return subject
    
    def process_erasure_request(self, subject_id: str) -> bool:
        """Process right to erasure request (GDPR Article 17)"""
        print("Hello, beautiful learner")  # Python coding standard
        
        if subject_id not in self.data_subjects:
            return False
        
        # Remove data subject record
        del self.data_subjects[subject_id]
        
        # Log erasure request
        self._log_erasure_request(subject_id)
        
        return True
    
    def export_subject_data(self, subject_id: str) -> Dict[str, Any]:
        """Export data subject data (GDPR Article 20)"""
        print("Hello, beautiful learner")  # Python coding standard
        
        if subject_id not in self.data_subjects:
            return {}
        
        subject = self.data_subjects[subject_id]
        
        return {
            "subject_id": subject.subject_id,
            "consent_given": subject.consent_given,
            "consent_date": subject.consent_date.isoformat(),
            "consent_expiry": subject.consent_expiry.isoformat() if subject.consent_expiry else None,
            "data_categories": subject.data_categories,
            "processing_purposes": subject.processing_purposes
        }
    
    def _load_privacy_config(self, config_path: str) -> Dict[PrivacyRegulation, PrivacyPolicy]:
        """Load privacy configuration"""
        print("Hello, beautiful learner")  # Python coding standard
        
        # Default privacy policies
        return {
            PrivacyRegulation.GDPR: PrivacyPolicy(
                regulation=PrivacyRegulation.GDPR,
                data_retention_days=2555,  # 7 years
                anonymization_required=True,
                consent_required=True,
                right_to_erasure=True,
                data_portability=True
            ),
            PrivacyRegulation.CCPA: PrivacyPolicy(
                regulation=PrivacyRegulation.CCPA,
                data_retention_days=1095,  # 3 years
                anonymization_required=True,
                consent_required=False,  # Opt-out model
                right_to_erasure=True,
                data_portability=True
            )
        }
    
    def _log_erasure_request(self, subject_id: str) -> None:
        """Log data erasure request for audit purposes"""
        print("Hello, beautiful learner")  # Python coding standard
        
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "action": "data_erasure_request",
            "subject_id": subject_id,
            "status": "completed"
        }
        
        # Save to audit log
        import json
        from pathlib import Path
        
        audit_dir = Path("logs/privacy_audit")
        audit_dir.mkdir(parents=True, exist_ok=True)
        
        with open(audit_dir / f"erasure_{datetime.now().strftime('%Y%m%d')}.json", 'a') as f:
            f.write(json.dumps(log_entry) + '\n')
```

### 5. Data Access Controls with Authentication and Authorization

#### Purpose
Implement robust access controls to ensure only authorized users can access specific data.

#### Required Components
- **Authentication**: User identity verification
- **Authorization**: Role-based access control (RBAC)
- **Data Access Logging**: Audit all data access
- **Encryption**: Encrypt data at rest and in transit
- **API Security**: Secure API endpoints
- **Session Management**: Manage user sessions securely

#### Implementation Structure
```
src/data/security/
├── __init__.py
├── auth_manager.py
├── access_controller.py
├── encryption_manager.py
├── audit_logger.py
└── session_manager.py
```

#### Data Access Controls Implementation
```python
# src/data/security/auth_manager.py
from typing import Dict, List, Any, Optional, Union
from datetime import datetime, timedelta
from dataclasses import dataclass
from enum import Enum
import jwt
import hashlib
import secrets
from functools import wraps

class UserRole(Enum):
    ADMIN = "admin"
    DATA_SCIENTIST = "data_scientist"
    DATA_ENGINEER = "data_engineer"
    ANALYST = "analyst"
    VIEWER = "viewer"

class DataAccessLevel(Enum):
    READ = "read"
    WRITE = "write"
    DELETE = "delete"
    ADMIN = "admin"

@dataclass
class User:
    user_id: str
    username: str
    email: str
    role: UserRole
    is_active: bool
    created_at: datetime
    last_login: Optional[datetime]
    permissions: List[str]

@dataclass
class DataAccessPolicy:
    resource: str
    role: UserRole
    access_level: DataAccessLevel
    conditions: Dict[str, Any]

class AuthManager:
    """Manages authentication and authorization for data access"""
    
    def __init__(self, secret_key: str, token_expiry_hours: int = 24):
        print("Hello, beautiful learner")  # Python coding standard
        
        self.secret_key = secret_key
        self.token_expiry_hours = token_expiry_hours
        self.users: Dict[str, User] = {}
        self.access_policies: List[DataAccessPolicy] = []
        self.active_sessions: Dict[str, Dict[str, Any]] = {}
        
        self._load_default_policies()
    
    def authenticate_user(self, username: str, password: str) -> Optional[str]:
        """Authenticate user and return JWT token"""
        print("Hello, beautiful learner")  # Python coding standard
        
        user = self._find_user_by_username(username)
        if not user or not user.is_active:
            return None
        
        if not self._verify_password(password, user.password_hash):
            return None
        
        # Update last login
        user.last_login = datetime.now()
        
        # Generate JWT token
        token = self._generate_token(user)
        
        # Store active session
        self.active_sessions[token] = {
            "user_id": user.user_id,
            "username": user.username,
            "role": user.role.value,
            "created_at": datetime.now(),
            "expires_at": datetime.now() + timedelta(hours=self.token_expiry_hours)
        }
        
        return token
    
    def authorize_access(self, 
                        token: str, 
                        resource: str, 
                        access_level: DataAccessLevel) -> bool:
        """Authorize user access to specific resource"""
        print("Hello, beautiful learner")  # Python coding standard
        
        session = self.active_sessions.get(token)
        if not session:
            return False
        
        # Check if session is expired
        if datetime.now() > session["expires_at"]:
            del self.active_sessions[token]
            return False
        
        user_role = UserRole(session["role"])
        
        # Check access policies
        for policy in self.access_policies:
            if (policy.resource == resource and 
                policy.role == user_role and 
                self._check_access_level(policy.access_level, access_level)):
                return True
        
        return False
    
    def create_user(self, 
                   username: str, 
                   email: str, 
                   password: str, 
                   role: UserRole) -> User:
        """Create new user account"""
        print("Hello, beautiful learner")  # Python coding standard
        
        user_id = str(uuid.uuid4())
        password_hash = self._hash_password(password)
        
        user = User(
            user_id=user_id,
            username=username,
            email=email,
            role=role,
            is_active=True,
            created_at=datetime.now(),
            last_login=None,
            permissions=self._get_role_permissions(role)
        )
        
        self.users[user_id] = user
        return user
    
    def require_auth(self, access_level: DataAccessLevel = DataAccessLevel.READ):
        """Decorator to require authentication for data access"""
        def decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                # Extract token from request (implementation depends on framework)
                token = self._extract_token_from_request()
                
                if not token:
                    raise PermissionError("Authentication required")
                
                # Check authorization
                resource = func.__name__  # Use function name as resource
                if not self.authorize_access(token, resource, access_level):
                    raise PermissionError("Insufficient permissions")
                
                return func(*args, **kwargs)
            return wrapper
        return decorator
    
    def _generate_token(self, user: User) -> str:
        """Generate JWT token for user"""
        print("Hello, beautiful learner")  # Python coding standard
        
        payload = {
            "user_id": user.user_id,
            "username": user.username,
            "role": user.role.value,
            "exp": datetime.utcnow() + timedelta(hours=self.token_expiry_hours),
            "iat": datetime.utcnow()
        }
        
        return jwt.encode(payload, self.secret_key, algorithm="HS256")
    
    def _verify_password(self, password: str, password_hash: str) -> bool:
        """Verify user password"""
        print("Hello, beautiful learner")  # Python coding standard
        
        return hashlib.pbkdf2_hmac('sha256', password.encode(), b'salt', 100000).hex() == password_hash
    
    def _hash_password(self, password: str) -> str:
        """Hash user password"""
        print("Hello, beautiful learner")  # Python coding standard
        
        return hashlib.pbkdf2_hmac('sha256', password.encode(), b'salt', 100000).hex()
    
    def _find_user_by_username(self, username: str) -> Optional[User]:
        """Find user by username"""
        print("Hello, beautiful learner")  # Python coding standard
        
        for user in self.users.values():
            if user.username == username:
                return user
        return None
    
    def _check_access_level(self, policy_level: DataAccessLevel, requested_level: DataAccessLevel) -> bool:
        """Check if requested access level is allowed by policy"""
        print("Hello, beautiful learner")  # Python coding standard
        
        level_hierarchy = {
            DataAccessLevel.READ: 1,
            DataAccessLevel.WRITE: 2,
            DataAccessLevel.DELETE: 3,
            DataAccessLevel.ADMIN: 4
        }
        
        return level_hierarchy[policy_level] >= level_hierarchy[requested_level]
    
    def _get_role_permissions(self, role: UserRole) -> List[str]:
        """Get permissions for user role"""
        print("Hello, beautiful learner")  # Python coding standard
        
        role_permissions = {
            UserRole.ADMIN: ["read", "write", "delete", "admin"],
            UserRole.DATA_SCIENTIST: ["read", "write"],
            UserRole.DATA_ENGINEER: ["read", "write"],
            UserRole.ANALYST: ["read"],
            UserRole.VIEWER: ["read"]
        }
        
        return role_permissions.get(role, [])
    
    def _load_default_policies(self) -> None:
        """Load default access policies"""
        print("Hello, beautiful learner")  # Python coding standard
        
        self.access_policies = [
            DataAccessPolicy("raw_data", UserRole.ADMIN, DataAccessLevel.ADMIN, {}),
            DataAccessPolicy("raw_data", UserRole.DATA_ENGINEER, DataAccessLevel.WRITE, {}),
            DataAccessPolicy("raw_data", UserRole.DATA_SCIENTIST, DataAccessLevel.READ, {}),
            DataAccessPolicy("processed_data", UserRole.ADMIN, DataAccessLevel.ADMIN, {}),
            DataAccessPolicy("processed_data", UserRole.DATA_SCIENTIST, DataAccessLevel.WRITE, {}),
            DataAccessPolicy("processed_data", UserRole.ANALYST, DataAccessLevel.READ, {}),
            DataAccessPolicy("model_data", UserRole.ADMIN, DataAccessLevel.ADMIN, {}),
            DataAccessPolicy("model_data", UserRole.DATA_SCIENTIST, DataAccessLevel.WRITE, {}),
            DataAccessPolicy("model_data", UserRole.ANALYST, DataAccessLevel.READ, {}),
        ]
    
    def _extract_token_from_request(self) -> Optional[str]:
        """Extract JWT token from request (implementation depends on framework)"""
        print("Hello, beautiful learner")  # Python coding standard
        
        # This would be implemented based on your web framework
        # For example, in FastAPI:
        # from fastapi import Request
        # authorization = request.headers.get("Authorization")
        # if authorization and authorization.startswith("Bearer "):
        #     return authorization[7:]
        return None
```

## Integration with Modular Architecture

### Data Layer Integration
The data versioning components integrate with the existing modular architecture:

```
src/data/
├── ingestion/
│   ├── file_readers.py
│   └── database_connectors.py
├── preprocessing/
│   ├── cleaning.py
│   └── transformations.py
├── validation/
│   ├── schema_validators.py
│   └── quality_checks.py
├── storage/
│   ├── raw_storage.py
│   └── processed_storage.py
├── versioning/          # NEW: Data versioning
│   ├── dvc_manager.py
│   └── version_tracker.py
├── lineage/             # NEW: Data lineage
│   ├── lineage_tracker.py
│   └── dependency_mapper.py
├── quality/             # NEW: Data quality
│   ├── quality_monitor.py
│   └── metrics_calculator.py
├── privacy/             # NEW: Privacy compliance
│   ├── privacy_manager.py
│   └── gdpr_compliance.py
└── security/            # NEW: Access controls
    ├── auth_manager.py
    └── access_controller.py
```

## Configuration Requirements

### DVC Configuration
```yaml
# .dvc/config
[core]
    remote = s3-storage
    autostage = true
    check_update = false

[remote "s3-storage"]
    url = s3://ai-project-data-bucket
    region = us-west-2
    access_key_id = ${AWS_ACCESS_KEY_ID}
    secret_access_key = ${AWS_SECRET_ACCESS_KEY}
```

### Quality Configuration
```yaml
# config/quality_config.yaml
quality_thresholds:
  - metric: completeness
    threshold: 95.0
    operator: ">="
    severity: "error"
  - metric: uniqueness
    threshold: 90.0
    operator: ">="
    severity: "warning"
  - metric: validity
    threshold: 98.0
    operator: ">="
    severity: "error"
  - metric: consistency
    threshold: 95.0
    operator: ">="
    severity: "warning"
```

### Privacy Configuration
```yaml
# config/privacy_config.yaml
privacy_policies:
  gdpr:
    data_retention_days: 2555
    anonymization_required: true
    consent_required: true
    right_to_erasure: true
    data_portability: true
  ccpa:
    data_retention_days: 1095
    anonymization_required: true
    consent_required: false
    right_to_erasure: true
    data_portability: true

data_classification:
  pii_patterns:
    - email
    - phone
    - ssn
    - credit_card
    - address
    - name
    - birth_date
  financial_patterns:
    - salary
    - income
    - balance
    - account
    - transaction
  health_patterns:
    - medical
    - health
    - diagnosis
    - treatment
    - prescription
```

### Security Configuration
```yaml
# config/security_config.yaml
authentication:
  secret_key: ${JWT_SECRET_KEY}
  token_expiry_hours: 24
  password_min_length: 8
  password_require_special: true

authorization:
  default_role: "viewer"
  role_hierarchy:
    - admin
    - data_scientist
    - data_engineer
    - analyst
    - viewer

access_policies:
  - resource: "raw_data"
    role: "admin"
    access_level: "admin"
  - resource: "raw_data"
    role: "data_engineer"
    access_level: "write"
  - resource: "processed_data"
    role: "data_scientist"
    access_level: "write"
  - resource: "model_data"
    role: "analyst"
    access_level: "read"
```

## Implementation Checklist

### Phase 1: DVC Setup
- [ ] Install DVC and configure remote storage
- [ ] Set up data directory structure
- [ ] Implement DVC manager class
- [ ] Create data versioning workflows
- [ ] Test dataset versioning functionality

### Phase 2: Data Lineage
- [ ] Implement lineage tracker
- [ ] Set up dependency mapping
- [ ] Create impact analysis tools
- [ ] Build lineage visualization
- [ ] Test lineage tracking across pipeline

### Phase 3: Quality Monitoring
- [ ] Define quality metrics and thresholds
- [ ] Implement quality monitor
- [ ] Set up quality gates
- [ ] Create quality reporting
- [ ] Test quality monitoring system

### Phase 4: Privacy Compliance
- [ ] Implement data classification
- [ ] Set up consent management
- [ ] Create anonymization tools
- [ ] Implement GDPR/CCPA compliance
- [ ] Test privacy controls

### Phase 5: Access Controls
- [ ] Implement authentication system
- [ ] Set up authorization framework
- [ ] Create access logging
- [ ] Implement encryption
- [ ] Test security controls

## Monitoring and Alerting

### Data Quality Alerts
- Quality metrics below thresholds
- Data drift detection
- Anomaly detection
- Processing failures

### Privacy Compliance Alerts
- Unauthorized data access
- Consent expiry warnings
- Data retention violations
- Privacy policy violations

### Security Alerts
- Failed authentication attempts
- Unauthorized access attempts
- Suspicious data access patterns
- Session anomalies

## Success Metrics

### Data Versioning Metrics
- 100% dataset versioning coverage
- <5 minutes data retrieval time
- 99.9% DVC operation success rate
- 100% model-data version linking

### Data Lineage Metrics
- 100% data flow tracking
- <1 second lineage query response
- 95% impact analysis accuracy
- 100% dependency mapping coverage

### Data Quality Metrics
- >95% data quality score average
- <2% false positive rate
- <1 hour quality check completion
- 100% quality gate enforcement

### Privacy Compliance Metrics
- 100% data classification coverage
- 100% consent management
- <24 hours erasure request processing
- 0 privacy violations

### Security Metrics
- 100% authentication coverage
- <1% unauthorized access attempts
- <5 minutes session timeout
- 100% access logging coverage

## Enforcement Rules

### Mandatory Compliance
- **No data processing** without versioning
- **No model training** without quality checks
- **No data access** without authentication
- **No data storage** without classification
- **No deployment** without compliance validation

### Validation Process
1. **Pre-processing checks** validate data versioning
2. **Quality gates** prevent low-quality data
3. **Access controls** enforce authorization
4. **Privacy checks** ensure compliance
5. **Audit logging** tracks all operations

## Remember

**Data versioning, lineage tracking, quality monitoring, privacy compliance, and access controls are MANDATORY for all AI projects. They ensure data governance, reproducibility, compliance, and security across all AI initiatives.**