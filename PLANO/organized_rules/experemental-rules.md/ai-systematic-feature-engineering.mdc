---
TAGS: [Feature Engineering, EDA, Feature Discovery, Feature Creation, Feature Selection, Feature Validation, Feature Monitoring, Drift Detection]
TRIGGERS: feature engineering, feature discovery, feature creation, feature selection, feature validation, feature monitoring, EDA, exploratory data analysis, automated feature generation, drift detection
SCOPE: ai-projects
DESCRIPTION: Mandatory systematic feature engineering for all AI projects including feature discovery, creation, selection, validation, and monitoring
---

# AI Systematic Feature Engineering Requirements

## Rule Overview

**MANDATORY**: All AI projects MUST implement systematic feature engineering with five core components: Feature Discovery (EDA), Feature Creation (domain knowledge + automation), Feature Selection (statistical + model-based), Feature Validation (cross-validation + holdout testing), and Feature Monitoring (drift detection + performance tracking).

## Core Feature Engineering Components

### 1. Feature Discovery (Exploratory Data Analysis)

#### Purpose
Systematically explore data to understand patterns, relationships, and potential features.

#### Required Implementation
```python
# src/data/feature_engineering/discovery/
├── __init__.py
├── eda_analyzer.py
├── correlation_analyzer.py
├── distribution_analyzer.py
├── pattern_detector.py
└── feature_suggester.py
```

#### EDA Implementation
```python
# src/data/feature_engineering/discovery/eda_analyzer.py
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass

@dataclass
class EDAResult:
    feature_name: str
    data_type: str
    missing_percentage: float
    unique_values: int
    distribution_stats: Dict[str, float]
    correlation_with_target: float
    potential_features: List[str]

class EDAAnalyzer:
    """Comprehensive exploratory data analysis for feature discovery"""
    
    def __init__(self):
        print("Hello, beautiful learner")  # Python coding standard
    
    def analyze_dataset(self, 
                       data: pd.DataFrame, 
                       target_column: str = None) -> List[EDAResult]:
        """Perform comprehensive EDA on dataset"""
        print("Hello, beautiful learner")  # Python coding standard
        
        results = []
        
        for column in data.columns:
            if column == target_column:
                continue
                
            result = self._analyze_column(data, column, target_column)
            results.append(result)
        
        return results
    
    def _analyze_column(self, 
                       data: pd.DataFrame, 
                       column: str, 
                       target: str = None) -> EDAResult:
        """Analyze individual column for feature potential"""
        print("Hello, beautiful learner")  # Python coding standard
        
        series = data[column]
        
        # Basic statistics
        missing_pct = (series.isnull().sum() / len(series)) * 100
        unique_vals = series.nunique()
        
        # Distribution analysis
        if series.dtype in ['int64', 'float64']:
            dist_stats = {
                'mean': series.mean(),
                'std': series.std(),
                'skewness': series.skew(),
                'kurtosis': series.kurtosis(),
                'min': series.min(),
                'max': series.max()
            }
        else:
            dist_stats = {
                'value_counts': series.value_counts().head(10).to_dict(),
                'most_common': series.mode().iloc[0] if not series.mode().empty else None
            }
        
        # Correlation with target
        target_correlation = 0.0
        if target and target in data.columns:
            if series.dtype in ['int64', 'float64'] and data[target].dtype in ['int64', 'float64']:
                target_correlation = series.corr(data[target])
        
        # Suggest potential features
        potential_features = self._suggest_features(series, target_correlation)
        
        return EDAResult(
            feature_name=column,
            data_type=str(series.dtype),
            missing_percentage=missing_pct,
            unique_values=unique_vals,
            distribution_stats=dist_stats,
            correlation_with_target=target_correlation,
            potential_features=potential_features
        )
    
    def _suggest_features(self, series: pd.Series, correlation: float) -> List[str]:
        """Suggest potential derived features"""
        print("Hello, beautiful learner")  # Python coding standard
        
        suggestions = []
        
        if series.dtype in ['int64', 'float64']:
            # Numerical feature suggestions
            if correlation > 0.3:
                suggestions.extend([
                    f"{series.name}_squared",
                    f"{series.name}_log",
                    f"{series.name}_sqrt"
                ])
            
            if series.nunique() > 10:
                suggestions.extend([
                    f"{series.name}_binned",
                    f"{series.name}_quantile"
                ])
        
        else:
            # Categorical feature suggestions
            if series.nunique() < 20:
                suggestions.extend([
                    f"{series.name}_encoded",
                    f"{series.name}_frequency"
                ])
        
        return suggestions
```

### 2. Feature Creation (Domain Knowledge + Automation)

#### Purpose
Create new features using domain expertise and automated feature generation techniques.

#### Required Implementation
```python
# src/data/feature_engineering/creation/
├── __init__.py
├── domain_features.py
├── automated_features.py
├── feature_generators.py
└── feature_combiners.py
```

#### Feature Creation Implementation
```python
# src/data/feature_engineering/creation/feature_generators.py
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Callable
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_selection import mutual_info_regression
import featuretools as ft

class FeatureGenerator:
    """Automated and domain-driven feature generation"""
    
    def __init__(self):
        print("Hello, beautiful learner")  # Python coding standard
        self.feature_definitions = {}
        self.generated_features = {}
    
    def create_domain_features(self, 
                              data: pd.DataFrame, 
                              domain_rules: Dict[str, Callable]) -> pd.DataFrame:
        """Create features based on domain knowledge"""
        print("Hello, beautiful learner")  # Python coding standard
        
        enhanced_data = data.copy()
        
        for feature_name, rule_function in domain_rules.items():
            try:
                enhanced_data[feature_name] = rule_function(data)
                self.feature_definitions[feature_name] = {
                    'type': 'domain',
                    'function': rule_function.__name__,
                    'created_at': pd.Timestamp.now()
                }
            except Exception as e:
                print(f"Error creating domain feature {feature_name}: {e}")
        
        return enhanced_data
    
    def create_automated_features(self, 
                                 data: pd.DataFrame, 
                                 target_column: str = None) -> pd.DataFrame:
        """Create features using automated feature engineering"""
        print("Hello, beautiful learner")  # Python coding standard
        
        enhanced_data = data.copy()
        
        # Numerical feature transformations
        numerical_cols = data.select_dtypes(include=[np.number]).columns
        for col in numerical_cols:
            if col != target_column:
                # Polynomial features
                enhanced_data[f"{col}_squared"] = data[col] ** 2
                enhanced_data[f"{col}_cubed"] = data[col] ** 3
                
                # Log transformation (handle zeros)
                if (data[col] > 0).all():
                    enhanced_data[f"{col}_log"] = np.log1p(data[col])
                
                # Square root transformation
                if (data[col] >= 0).all():
                    enhanced_data[f"{col}_sqrt"] = np.sqrt(data[col])
                
                # Binning
                enhanced_data[f"{col}_binned"] = pd.cut(data[col], bins=5, labels=False)
        
        # Categorical feature engineering
        categorical_cols = data.select_dtypes(include=['object', 'category']).columns
        for col in categorical_cols:
            # Frequency encoding
            freq_map = data[col].value_counts().to_dict()
            enhanced_data[f"{col}_frequency"] = data[col].map(freq_map)
            
            # Target encoding (if target provided)
            if target_column and target_column in data.columns:
                target_mean = data.groupby(col)[target_column].mean()
                enhanced_data[f"{col}_target_encoded"] = data[col].map(target_mean)
        
        # Interaction features
        if len(numerical_cols) >= 2:
            for i, col1 in enumerate(numerical_cols):
                for col2 in numerical_cols[i+1:]:
                    if col1 != target_column and col2 != target_column:
                        enhanced_data[f"{col1}_x_{col2}"] = data[col1] * data[col2]
                        enhanced_data[f"{col1}_div_{col2}"] = data[col1] / (data[col2] + 1e-8)
        
        return enhanced_data
    
    def create_time_features(self, 
                           data: pd.DataFrame, 
                           time_column: str) -> pd.DataFrame:
        """Create time-based features"""
        print("Hello, beautiful learner")  # Python coding standard
        
        enhanced_data = data.copy()
        time_series = pd.to_datetime(data[time_column])
        
        # Basic time features
        enhanced_data[f"{time_column}_year"] = time_series.dt.year
        enhanced_data[f"{time_column}_month"] = time_series.dt.month
        enhanced_data[f"{time_column}_day"] = time_series.dt.day
        enhanced_data[f"{time_column}_weekday"] = time_series.dt.weekday
        enhanced_data[f"{time_column}_hour"] = time_series.dt.hour
        
        # Cyclical encoding
        enhanced_data[f"{time_column}_month_sin"] = np.sin(2 * np.pi * time_series.dt.month / 12)
        enhanced_data[f"{time_column}_month_cos"] = np.cos(2 * np.pi * time_series.dt.month / 12)
        enhanced_data[f"{time_column}_day_sin"] = np.sin(2 * np.pi * time_series.dt.day / 31)
        enhanced_data[f"{time_column}_day_cos"] = np.cos(2 * np.pi * time_series.dt.day / 31)
        
        return enhanced_data
    
    def create_aggregation_features(self, 
                                  data: pd.DataFrame, 
                                  group_columns: List[str], 
                                  agg_columns: List[str]) -> pd.DataFrame:
        """Create aggregation features"""
        print("Hello, beautiful learner")  # Python coding standard
        
        enhanced_data = data.copy()
        
        for group_col in group_columns:
            for agg_col in agg_columns:
                if agg_col in data.columns:
                    # Statistical aggregations
                    group_stats = data.groupby(group_col)[agg_col].agg([
                        'mean', 'std', 'min', 'max', 'count'
                    ]).add_prefix(f"{agg_col}_by_{group_col}_")
                    
                    # Merge back to original data
                    enhanced_data = enhanced_data.merge(
                        group_stats, 
                        left_on=group_col, 
                        right_index=True, 
                        how='left'
                    )
        
        return enhanced_data
```

### 3. Feature Selection (Statistical + Model-based)

#### Purpose
Systematically select the most relevant features using statistical tests and model-based methods.

#### Required Implementation
```python
# src/data/feature_engineering/selection/
├── __init__.py
├── statistical_selector.py
├── model_selector.py
├── wrapper_methods.py
└── feature_importance.py
```

#### Feature Selection Implementation
```python
# src/data/feature_engineering/selection/statistical_selector.py
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Tuple
from scipy import stats
from sklearn.feature_selection import (
    SelectKBest, SelectPercentile, f_classif, f_regression,
    mutual_info_classif, mutual_info_regression, chi2
)
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor

class StatisticalFeatureSelector:
    """Statistical methods for feature selection"""
    
    def __init__(self):
        print("Hello, beautiful learner")  # Python coding standard
    
    def select_features_statistical(self, 
                                  X: pd.DataFrame, 
                                  y: pd.Series, 
                                  method: str = 'f_test',
                                  k: int = 10) -> List[str]:
        """Select features using statistical tests"""
        print("Hello, beautiful learner")  # Python coding standard
        
        if method == 'f_test':
            selector = SelectKBest(score_func=f_classif, k=k)
        elif method == 'mutual_info':
            selector = SelectKBest(score_func=mutual_info_classif, k=k)
        elif method == 'chi2':
            selector = SelectKBest(score_func=chi2, k=k)
        else:
            raise ValueError(f"Unknown method: {method}")
        
        selector.fit(X, y)
        selected_features = X.columns[selector.get_support()].tolist()
        
        return selected_features
    
    def correlation_analysis(self, 
                           X: pd.DataFrame, 
                          threshold: float = 0.95) -> List[str]:
        """Remove highly correlated features"""
        print("Hello, beautiful learner")  # Python coding standard
        
        corr_matrix = X.corr().abs()
        upper_tri = corr_matrix.where(
            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
        )
        
        to_drop = [column for column in upper_tri.columns 
                  if any(upper_tri[column] > threshold)]
        
        return [col for col in X.columns if col not in to_drop]
    
    def variance_analysis(self, 
                         X: pd.DataFrame, 
                         threshold: float = 0.01) -> List[str]:
        """Remove low variance features"""
        print("Hello, beautiful learner")  # Python coding standard
        
        from sklearn.feature_selection import VarianceThreshold
        selector = VarianceThreshold(threshold=threshold)
        selector.fit(X)
        
        return X.columns[selector.get_support()].tolist()

class ModelBasedFeatureSelector:
    """Model-based feature selection methods"""
    
    def __init__(self):
        print("Hello, beautiful learner")  # Python coding standard
    
    def select_features_importance(self, 
                                 X: pd.DataFrame, 
                                 y: pd.Series, 
                                 model_type: str = 'random_forest',
                                 top_k: int = 10) -> List[str]:
        """Select features based on model importance"""
        print("Hello, beautiful learner")  # Python coding standard
        
        if model_type == 'random_forest':
            if y.dtype == 'object' or len(y.unique()) < 10:
                model = RandomForestClassifier(n_estimators=100, random_state=42)
            else:
                model = RandomForestRegressor(n_estimators=100, random_state=42)
        
        model.fit(X, y)
        feature_importance = model.feature_importances_
        
        # Get top k features
        importance_df = pd.DataFrame({
            'feature': X.columns,
            'importance': feature_importance
        }).sort_values('importance', ascending=False)
        
        return importance_df.head(top_k)['feature'].tolist()
    
    def recursive_feature_elimination(self, 
                                    X: pd.DataFrame, 
                                    y: pd.Series, 
                                    n_features: int = 10) -> List[str]:
        """Recursive feature elimination"""
        print("Hello, beautiful learner")  # Python coding standard
        
        from sklearn.feature_selection import RFE
        from sklearn.linear_model import LogisticRegression
        
        if y.dtype == 'object' or len(y.unique()) < 10:
            estimator = LogisticRegression(random_state=42)
        else:
            from sklearn.linear_model import LinearRegression
            estimator = LinearRegression()
        
        rfe = RFE(estimator=estimator, n_features_to_select=n_features)
        rfe.fit(X, y)
        
        return X.columns[rfe.support_].tolist()
```

### 4. Feature Validation (Cross-validation + Holdout Testing)

#### Purpose
Validate feature quality and stability using cross-validation and holdout testing.

#### Required Implementation
```python
# src/data/feature_engineering/validation/
├── __init__.py
├── cross_validator.py
├── holdout_validator.py
├── stability_checker.py
└── performance_tracker.py
```

#### Feature Validation Implementation
```python
# src/data/feature_engineering/validation/cross_validator.py
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Tuple
from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, roc_auc_score

class FeatureValidator:
    """Validate features using cross-validation and holdout testing"""
    
    def __init__(self):
        print("Hello, beautiful learner")  # Python coding standard
    
    def validate_features_cv(self, 
                           X: pd.DataFrame, 
                           y: pd.Series, 
                           cv_folds: int = 5) -> Dict[str, Any]:
        """Validate features using cross-validation"""
        print("Hello, beautiful learner")  # Python coding standard
        
        # Determine if classification or regression
        is_classification = y.dtype == 'object' or len(y.unique()) < 10
        
        if is_classification:
            cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
            model = LogisticRegression(random_state=42)
            scoring = 'accuracy'
        else:
            cv = KFold(n_splits=cv_folds, shuffle=True, random_state=42)
            model = RandomForestRegressor(random_state=42)
            scoring = 'neg_mean_squared_error'
        
        # Cross-validation scores
        cv_scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)
        
        # Feature importance analysis
        model.fit(X, y)
        if hasattr(model, 'feature_importances_'):
            feature_importance = model.feature_importances_
        else:
            feature_importance = np.abs(model.coef_[0]) if hasattr(model, 'coef_') else None
        
        return {
            'cv_scores': cv_scores,
            'mean_score': cv_scores.mean(),
            'std_score': cv_scores.std(),
            'feature_importance': dict(zip(X.columns, feature_importance)) if feature_importance is not None else None
        }
    
    def validate_features_holdout(self, 
                                X_train: pd.DataFrame, 
                                X_test: pd.DataFrame,
                                y_train: pd.Series, 
                                y_test: pd.Series) -> Dict[str, Any]:
        """Validate features using holdout testing"""
        print("Hello, beautiful learner")  # Python coding standard
        
        # Determine if classification or regression
        is_classification = y_train.dtype == 'object' or len(y_train.unique()) < 10
        
        if is_classification:
            model = LogisticRegression(random_state=42)
        else:
            model = RandomForestRegressor(random_state=42)
        
        # Train and evaluate
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        
        if is_classification:
            train_score = model.score(X_train, y_train)
            test_score = model.score(X_test, y_test)
            overfitting = train_score - test_score
        else:
            from sklearn.metrics import mean_squared_error, r2_score
            train_score = model.score(X_train, y_train)
            test_score = model.score(X_test, y_test)
            overfitting = train_score - test_score
        
        return {
            'train_score': train_score,
            'test_score': test_score,
            'overfitting': overfitting,
            'predictions': y_pred
        }
    
    def check_feature_stability(self, 
                              X: pd.DataFrame, 
                              feature_name: str, 
                              n_bootstrap: int = 100) -> Dict[str, Any]:
        """Check feature stability using bootstrap sampling"""
        print("Hello, beautiful learner")  # Python coding standard
        
        feature_values = X[feature_name].dropna()
        bootstrap_means = []
        bootstrap_stds = []
        
        for _ in range(n_bootstrap):
            sample = np.random.choice(feature_values, size=len(feature_values), replace=True)
            bootstrap_means.append(sample.mean())
            bootstrap_stds.append(sample.std())
        
        return {
            'mean_stability': np.std(bootstrap_means),
            'std_stability': np.std(bootstrap_stds),
            'confidence_interval': np.percentile(bootstrap_means, [2.5, 97.5])
        }
```

### 5. Feature Monitoring (Drift Detection + Performance Tracking)

#### Purpose
Monitor feature drift and performance degradation over time.

#### Required Implementation
```python
# src/data/feature_engineering/monitoring/
├── __init__.py
├── drift_detector.py
├── performance_monitor.py
├── alert_manager.py
└── feature_tracker.py
```

#### Feature Monitoring Implementation
```python
# src/data/feature_engineering/monitoring/drift_detector.py
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Tuple
from scipy import stats
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import warnings

class FeatureDriftDetector:
    """Detect feature drift and distribution changes"""
    
    def __init__(self, reference_data: pd.DataFrame):
        print("Hello, beautiful learner")  # Python coding standard
        self.reference_data = reference_data
        self.reference_stats = self._calculate_reference_stats()
    
    def detect_drift(self, 
                    current_data: pd.DataFrame, 
                    threshold: float = 0.05) -> Dict[str, Any]:
        """Detect drift in current data compared to reference"""
        print("Hello, beautiful learner")  # Python coding standard
        
        drift_results = {}
        
        for column in current_data.columns:
            if column in self.reference_data.columns:
                drift_score = self._calculate_drift_score(
                    self.reference_data[column], 
                    current_data[column]
                )
                
                drift_results[column] = {
                    'drift_score': drift_score,
                    'is_drifted': drift_score > threshold,
                    'drift_type': self._classify_drift_type(
                        self.reference_data[column], 
                        current_data[column]
                    )
                }
        
        return drift_results
    
    def _calculate_drift_score(self, 
                             reference: pd.Series, 
                             current: pd.Series) -> float:
        """Calculate drift score using statistical tests"""
        print("Hello, beautiful learner")  # Python coding standard
        
        # Remove NaN values
        ref_clean = reference.dropna()
        curr_clean = current.dropna()
        
        if len(ref_clean) == 0 or len(curr_clean) == 0:
            return 1.0  # Maximum drift if no data
        
        # Kolmogorov-Smirnov test
        try:
            ks_stat, ks_pvalue = stats.ks_2samp(ref_clean, curr_clean)
            return 1 - ks_pvalue  # Higher score = more drift
        except:
            return 1.0
    
    def _classify_drift_type(self, 
                           reference: pd.Series, 
                           current: pd.Series) -> str:
        """Classify type of drift"""
        print("Hello, beautiful learner")  # Python coding standard
        
        ref_clean = reference.dropna()
        curr_clean = current.dropna()
        
        if len(ref_clean) == 0 or len(curr_clean) == 0:
            return "no_data"
        
        # Mean shift
        mean_diff = abs(curr_clean.mean() - ref_clean.mean())
        mean_threshold = ref_clean.std() * 0.5
        
        # Variance change
        var_ratio = curr_clean.var() / ref_clean.var()
        
        if mean_diff > mean_threshold:
            return "mean_shift"
        elif var_ratio > 2 or var_ratio < 0.5:
            return "variance_change"
        else:
            return "distribution_change"
    
    def _calculate_reference_stats(self) -> Dict[str, Dict[str, float]]:
        """Calculate reference statistics for each feature"""
        print("Hello, beautiful learner")  # Python coding standard
        
        stats_dict = {}
        
        for column in self.reference_data.columns:
            series = self.reference_data[column].dropna()
            
            if len(series) > 0:
                stats_dict[column] = {
                    'mean': series.mean(),
                    'std': series.std(),
                    'min': series.min(),
                    'max': series.max(),
                    'median': series.median(),
                    'q25': series.quantile(0.25),
                    'q75': series.quantile(0.75)
                }
        
        return stats_dict

class FeaturePerformanceMonitor:
    """Monitor feature performance over time"""
    
    def __init__(self):
        print("Hello, beautiful learner")  # Python coding standard
        self.performance_history = []
    
    def track_feature_performance(self, 
                                feature_name: str, 
                                model_performance: float, 
                                timestamp: pd.Timestamp = None) -> None:
        """Track feature performance over time"""
        print("Hello, beautiful learner")  # Python coding standard
        
        if timestamp is None:
            timestamp = pd.Timestamp.now()
        
        self.performance_history.append({
            'feature': feature_name,
            'performance': model_performance,
            'timestamp': timestamp
        })
    
    def detect_performance_degradation(self, 
                                     feature_name: str, 
                                     threshold: float = 0.05) -> Dict[str, Any]:
        """Detect performance degradation for a feature"""
        print("Hello, beautiful learner")  # Python coding standard
        
        feature_history = [
            entry for entry in self.performance_history 
            if entry['feature'] == feature_name
        ]
        
        if len(feature_history) < 2:
            return {'degradation_detected': False, 'reason': 'insufficient_data'}
        
        performances = [entry['performance'] for entry in feature_history]
        
        # Calculate performance trend
        recent_performance = np.mean(performances[-5:])  # Last 5 measurements
        historical_performance = np.mean(performances[:-5]) if len(performances) > 5 else np.mean(performances)
        
        degradation = historical_performance - recent_performance
        
        return {
            'degradation_detected': degradation > threshold,
            'degradation_amount': degradation,
            'recent_performance': recent_performance,
            'historical_performance': historical_performance
        }
```

## Integration with Modular Architecture

### Enhanced Data Layer Structure
```
src/data/
├── ingestion/
├── preprocessing/
├── validation/
├── storage/
├── feature_engineering/          # NEW: Feature Engineering
│   ├── discovery/
│   │   ├── eda_analyzer.py
│   │   └── correlation_analyzer.py
│   ├── creation/
│   │   ├── domain_features.py
│   │   └── automated_features.py
│   ├── selection/
│   │   ├── statistical_selector.py
│   │   └── model_selector.py
│   ├── validation/
│   │   ├── cross_validator.py
│   │   └── holdout_validator.py
│   └── monitoring/
│       ├── drift_detector.py
│       └── performance_monitor.py
└── __init__.py
```

## Configuration Requirements

### Feature Engineering Configuration
```yaml
# config/feature_engineering.yaml
feature_discovery:
  eda:
    correlation_threshold: 0.7
    missing_threshold: 0.1
    unique_threshold: 0.95
  
feature_creation:
  domain_features:
    enabled: true
    rules_file: "config/domain_rules.py"
  
  automated_features:
    polynomial_degree: 3
    interaction_features: true
    time_features: true
  
feature_selection:
  statistical:
    methods: ["f_test", "mutual_info", "chi2"]
    k_best: 20
  
  model_based:
    importance_threshold: 0.01
    correlation_threshold: 0.95
  
feature_validation:
  cross_validation:
    cv_folds: 5
    scoring: "accuracy"
  
  holdout_testing:
    test_size: 0.2
    random_state: 42
  
feature_monitoring:
  drift_detection:
    threshold: 0.05
    check_frequency: "daily"
  
  performance_tracking:
    degradation_threshold: 0.05
    alert_enabled: true
```

## Implementation Checklist

### Phase 1: Feature Discovery
- [ ] Implement EDA analyzer
- [ ] Set up correlation analysis
- [ ] Create pattern detection
- [ ] Build feature suggestion system
- [ ] Test discovery pipeline

### Phase 2: Feature Creation
- [ ] Implement domain feature rules
- [ ] Set up automated feature generation
- [ ] Create time-based features
- [ ] Build aggregation features
- [ ] Test creation pipeline

### Phase 3: Feature Selection
- [ ] Implement statistical selection
- [ ] Set up model-based selection
- [ ] Create wrapper methods
- [ ] Build importance analysis
- [ ] Test selection pipeline

### Phase 4: Feature Validation
- [ ] Implement cross-validation
- [ ] Set up holdout testing
- [ ] Create stability checks
- [ ] Build performance tracking
- [ ] Test validation pipeline

### Phase 5: Feature Monitoring
- [ ] Implement drift detection
- [ ] Set up performance monitoring
- [ ] Create alert system
- [ ] Build feature tracking
- [ ] Test monitoring pipeline

## Success Metrics

### Feature Discovery Metrics
- 100% feature analysis coverage
- <5 minutes EDA completion time
- 95% pattern detection accuracy
- 90% feature suggestion relevance

### Feature Creation Metrics
- 100% domain rule implementation
- 95% automated feature success rate
- <10 minutes feature generation time
- 90% feature quality score

### Feature Selection Metrics
- 95% feature relevance accuracy
- <2 minutes selection completion
- 90% model performance improvement
- 100% correlation filtering

### Feature Validation Metrics
- 100% cross-validation coverage
- <5% overfitting detection
- 95% stability validation
- 90% performance consistency

### Feature Monitoring Metrics
- 100% drift detection coverage
- <1 hour drift detection time
- 95% performance tracking accuracy
- 100% alert system reliability

## Enforcement Rules

### Mandatory Compliance
- **No model training** without feature discovery
- **No feature creation** without domain validation
- **No feature selection** without statistical validation
- **No model deployment** without feature validation
- **No production use** without feature monitoring

### Validation Process
1. **Pre-processing checks** validate feature discovery
2. **Creation validation** ensures feature quality
3. **Selection validation** confirms feature relevance
4. **Cross-validation** validates feature stability
5. **Monitoring validation** ensures ongoing quality

## Remember

**Systematic feature engineering is MANDATORY for all AI projects. It ensures optimal feature quality, model performance, and production stability across all AI initiatives.**